2026-01-28 16:55:55,143 - __main__ - INFO - Log file: ./output/train_20260128_165555.log
2026-01-28 16:55:55,143 - __main__ - WARNING - CUDA not available, falling back to CPU
2026-01-28 16:55:55,143 - __main__ - INFO - ============================================================
2026-01-28 16:55:55,144 - __main__ - INFO - RVC-LoRA End-to-End Training
2026-01-28 16:55:55,144 - __main__ - INFO - ============================================================
2026-01-28 16:55:55,144 - __main__ - INFO - Input directory: ./download/base_voice
2026-01-28 16:55:55,144 - __main__ - INFO - Output directory: ./output
2026-01-28 16:55:55,144 - __main__ - INFO - Base model: ./download/pretrained_v2/f0G40k.pth
2026-01-28 16:55:55,144 - __main__ - INFO - Sample rate: 40000
2026-01-28 16:55:55,144 - __main__ - INFO - Version: v2
2026-01-28 16:55:55,144 - __main__ - INFO - Device: cpu
2026-01-28 16:55:55,144 - __main__ - INFO - LoRA rank: 8, alpha: 16
2026-01-28 16:55:55,144 - __main__ - INFO - ============================================================
2026-01-28 16:55:55,144 - __main__ - INFO - Skipping preprocessing, found 47 existing samples
2026-01-28 16:55:55,144 - __main__ - INFO - 
============================================================
2026-01-28 16:55:55,144 - __main__ - INFO - Step 2: Loading model and injecting LoRA
2026-01-28 16:55:55,144 - __main__ - INFO - ============================================================
2026-01-28 16:55:55,194 - models.synthesizer_lora - INFO - Using default config for pretrained model (sr=40000, version=v2)
2026-01-28 16:55:55,480 - models.synthesizer_lora - INFO - Injecting LoRA into Synthesizer...
2026-01-28 16:55:55,538 - models.synthesizer_lora - INFO - LoRA injected into dec (Generator)
2026-01-28 16:55:55,539 - models.synthesizer_lora - INFO - Frozen 36,417,922 parameters, 384,768 LoRA parameters trainable
2026-01-28 16:55:55,545 - __main__ - INFO - Total parameters: 36,802,690
2026-01-28 16:55:55,545 - __main__ - INFO - LoRA parameters: 384,768 (1.05%)
2026-01-28 16:55:55,545 - __main__ - INFO - 
============================================================
2026-01-28 16:55:55,545 - __main__ - INFO - Step 3: Creating data loader
2026-01-28 16:55:55,545 - __main__ - INFO - ============================================================
2026-01-28 16:55:55,545 - training.data_loader - INFO - Auto-detected preprocessed dataset in ./output/preprocessed/training_data
2026-01-28 16:55:55,545 - training.data_loader - INFO - PreprocessedDataset: Found 47 samples
2026-01-28 16:55:55,545 - __main__ - INFO - Data loader created: 23 batches
2026-01-28 16:55:55,545 - __main__ - INFO - 
============================================================
2026-01-28 16:55:55,545 - __main__ - INFO - Step 4: Training LoRA
2026-01-28 16:55:55,546 - __main__ - INFO - ============================================================
2026-01-28 16:55:55,947 - training.train_lora - INFO - Optimizer setup with 152 parameter groups
2026-01-28 16:55:55,948 - training.train_lora - INFO - Starting training from epoch 0
2026-01-28 16:55:55,948 - training.train_lora - INFO - Total epochs: 50
2026-01-28 16:55:55,948 - training.train_lora - INFO - Batch size: 2
2026-01-28 16:55:55,948 - training.train_lora - INFO - Learning rate: 0.0001
2026-01-28 16:55:58,184 - training.train_lora - INFO - Epoch 0 [0/23] Step 0 LR: 0.000100 loss_total: 0.1640 loss_reconstruction: 0.1640 
2026-01-28 16:56:13,080 - training.train_lora - INFO - Epoch 0 [10/23] Step 10 LR: 0.000100 loss_total: 0.0918 loss_reconstruction: 0.0918 
2026-01-28 16:56:30,040 - training.train_lora - INFO - Epoch 0 [20/23] Step 20 LR: 0.000100 loss_total: 0.1351 loss_reconstruction: 0.1351 
2026-01-28 16:56:53,386 - training.train_lora - INFO - Epoch 0 completed in 57.44s - Loss: 0.1165
2026-01-28 16:57:05,991 - training.train_lora - INFO - Epoch 1 [7/23] Step 30 LR: 0.000100 loss_total: 0.0783 loss_reconstruction: 0.0783 
2026-01-28 16:57:23,054 - training.train_lora - INFO - Epoch 1 [17/23] Step 40 LR: 0.000100 loss_total: 0.1139 loss_reconstruction: 0.1139 
2026-01-28 16:57:51,316 - training.train_lora - INFO - Epoch 1 completed in 57.93s - Loss: 0.1107
2026-01-28 16:57:58,735 - training.train_lora - INFO - Epoch 2 [4/23] Step 50 LR: 0.000100 loss_total: 0.1032 loss_reconstruction: 0.1032 
2026-01-28 16:58:15,599 - training.train_lora - INFO - Epoch 2 [14/23] Step 60 LR: 0.000100 loss_total: 0.1111 loss_reconstruction: 0.1111 
2026-01-28 16:58:48,692 - training.train_lora - INFO - Epoch 2 completed in 57.38s - Loss: 0.1228
2026-01-28 16:58:52,326 - training.train_lora - INFO - Epoch 3 [1/23] Step 70 LR: 0.000100 loss_total: 0.1100 loss_reconstruction: 0.1100 
2026-01-28 16:59:08,201 - training.train_lora - INFO - Epoch 3 [11/23] Step 80 LR: 0.000100 loss_total: 0.1479 loss_reconstruction: 0.1479 
2026-01-28 16:59:24,461 - training.train_lora - INFO - Epoch 3 [21/23] Step 90 LR: 0.000100 loss_total: 0.1357 loss_reconstruction: 0.1357 
2026-01-28 16:59:46,082 - training.train_lora - INFO - Epoch 3 completed in 57.39s - Loss: 0.0988
2026-01-28 17:00:00,739 - training.train_lora - INFO - Epoch 4 [8/23] Step 100 LR: 0.000100 loss_total: 0.0735 loss_reconstruction: 0.0735 
2026-01-28 17:00:16,916 - training.train_lora - INFO - Epoch 4 [18/23] Step 110 LR: 0.000100 loss_total: 0.1334 loss_reconstruction: 0.1334 
2026-01-28 17:00:43,395 - training.train_lora - INFO - Epoch 4 completed in 57.31s - Loss: 0.1053
2026-01-28 17:00:52,591 - training.train_lora - INFO - Epoch 5 [5/23] Step 120 LR: 0.000100 loss_total: 0.0668 loss_reconstruction: 0.0668 
2026-01-28 17:01:09,390 - training.train_lora - INFO - Epoch 5 [15/23] Step 130 LR: 0.000100 loss_total: 0.1055 loss_reconstruction: 0.1055 
2026-01-28 17:01:40,849 - training.train_lora - INFO - Epoch 5 completed in 57.45s - Loss: 0.1093
2026-01-28 17:01:45,784 - training.train_lora - INFO - Epoch 6 [2/23] Step 140 LR: 0.000099 loss_total: 0.1001 loss_reconstruction: 0.1001 
2026-01-28 17:02:02,746 - training.train_lora - INFO - Epoch 6 [12/23] Step 150 LR: 0.000099 loss_total: 0.1318 loss_reconstruction: 0.1318 
2026-01-28 17:02:19,467 - training.train_lora - INFO - Epoch 6 [22/23] Step 160 LR: 0.000099 loss_total: 0.1180 loss_reconstruction: 0.1180 
2026-01-28 17:02:39,474 - training.train_lora - INFO - Epoch 6 completed in 58.62s - Loss: 0.1221
2026-01-28 17:02:56,584 - training.train_lora - INFO - Epoch 7 [9/23] Step 170 LR: 0.000099 loss_total: 0.1387 loss_reconstruction: 0.1387 
2026-01-28 17:03:13,188 - training.train_lora - INFO - Epoch 7 [19/23] Step 180 LR: 0.000099 loss_total: 0.0734 loss_reconstruction: 0.0734 
2026-01-28 17:03:38,186 - training.train_lora - INFO - Epoch 7 completed in 58.71s - Loss: 0.1081
2026-01-28 17:03:50,672 - training.train_lora - INFO - Epoch 8 [6/23] Step 190 LR: 0.000099 loss_total: 0.2966 loss_reconstruction: 0.2966 
2026-01-28 17:04:08,124 - training.train_lora - INFO - Epoch 8 [16/23] Step 200 LR: 0.000099 loss_total: 0.0703 loss_reconstruction: 0.0703 
2026-01-28 17:04:38,232 - training.train_lora - INFO - Epoch 8 completed in 60.05s - Loss: 0.1126
2026-01-28 17:04:45,469 - training.train_lora - INFO - Epoch 9 [3/23] Step 210 LR: 0.000099 loss_total: 0.2011 loss_reconstruction: 0.2011 
2026-01-28 17:05:03,268 - training.train_lora - INFO - Epoch 9 [13/23] Step 220 LR: 0.000099 loss_total: 0.0872 loss_reconstruction: 0.0872 
2026-01-28 17:05:38,212 - training.train_lora - INFO - Epoch 9 completed in 59.98s - Loss: 0.1077
2026-01-28 17:05:38,229 - training.train_lora - INFO - Saved LoRA checkpoint: ./output/checkpoints/lora_epoch_9.pth
2026-01-28 17:05:38,234 - training.train_lora - INFO - Saved best model: ./output/checkpoints/lora_best.pth
2026-01-28 17:05:40,829 - training.train_lora - INFO - Epoch 10 [0/23] Step 230 LR: 0.000099 loss_total: 0.0547 loss_reconstruction: 0.0547 
2026-01-28 17:05:58,330 - training.train_lora - INFO - Epoch 10 [10/23] Step 240 LR: 0.000099 loss_total: 0.1253 loss_reconstruction: 0.1253 
2026-01-28 17:06:15,378 - training.train_lora - INFO - Epoch 10 [20/23] Step 250 LR: 0.000099 loss_total: 0.1506 loss_reconstruction: 0.1506 
2026-01-28 17:06:38,802 - training.train_lora - INFO - Epoch 10 completed in 60.56s - Loss: 0.1151
2026-01-28 17:06:54,189 - training.train_lora - INFO - Epoch 11 [7/23] Step 260 LR: 0.000099 loss_total: 0.1345 loss_reconstruction: 0.1345 
2026-01-28 17:07:11,668 - training.train_lora - INFO - Epoch 11 [17/23] Step 270 LR: 0.000099 loss_total: 0.1020 loss_reconstruction: 0.1020 
2026-01-28 17:07:40,066 - training.train_lora - INFO - Epoch 11 completed in 61.26s - Loss: 0.1165
2026-01-28 17:07:50,360 - training.train_lora - INFO - Epoch 12 [4/23] Step 280 LR: 0.000099 loss_total: 0.1907 loss_reconstruction: 0.1907 
2026-01-28 17:08:07,284 - training.train_lora - INFO - Epoch 12 [14/23] Step 290 LR: 0.000099 loss_total: 0.1266 loss_reconstruction: 0.1266 
2026-01-28 17:08:41,158 - training.train_lora - INFO - Epoch 12 completed in 61.09s - Loss: 0.1120
2026-01-28 17:08:46,042 - training.train_lora - INFO - Epoch 13 [1/23] Step 300 LR: 0.000099 loss_total: 0.1593 loss_reconstruction: 0.1593 
2026-01-28 17:09:05,449 - training.train_lora - INFO - Epoch 13 [11/23] Step 310 LR: 0.000099 loss_total: 0.1014 loss_reconstruction: 0.1014 
2026-01-28 17:09:24,274 - training.train_lora - INFO - Epoch 13 [21/23] Step 320 LR: 0.000099 loss_total: 0.0974 loss_reconstruction: 0.0974 
2026-01-28 17:09:46,140 - training.train_lora - INFO - Epoch 13 completed in 64.98s - Loss: 0.1175
2026-01-28 17:10:03,806 - training.train_lora - INFO - Epoch 14 [8/23] Step 330 LR: 0.000099 loss_total: 0.1341 loss_reconstruction: 0.1341 
2026-01-28 17:10:21,436 - training.train_lora - INFO - Epoch 14 [18/23] Step 340 LR: 0.000099 loss_total: 0.1319 loss_reconstruction: 0.1319 
2026-01-28 17:10:48,471 - training.train_lora - INFO - Epoch 14 completed in 62.33s - Loss: 0.1071
2026-01-28 17:11:01,955 - training.train_lora - INFO - Epoch 15 [5/23] Step 350 LR: 0.000099 loss_total: 0.1682 loss_reconstruction: 0.1682 
2026-01-28 17:11:21,191 - training.train_lora - INFO - Epoch 15 [15/23] Step 360 LR: 0.000099 loss_total: 0.1385 loss_reconstruction: 0.1385 
2026-01-28 17:11:54,611 - training.train_lora - INFO - Epoch 15 completed in 66.14s - Loss: 0.1052
2026-01-28 17:12:02,733 - training.train_lora - INFO - Epoch 16 [2/23] Step 370 LR: 0.000098 loss_total: 0.1111 loss_reconstruction: 0.1111 
2026-01-28 17:12:20,317 - training.train_lora - INFO - Epoch 16 [12/23] Step 380 LR: 0.000098 loss_total: 0.1257 loss_reconstruction: 0.1257 
2026-01-28 17:12:38,257 - training.train_lora - INFO - Epoch 16 [22/23] Step 390 LR: 0.000098 loss_total: 0.0459 loss_reconstruction: 0.0459 
2026-01-28 17:12:58,264 - training.train_lora - INFO - Epoch 16 completed in 63.65s - Loss: 0.1082
2026-01-28 17:13:19,860 - training.train_lora - INFO - Epoch 17 [9/23] Step 400 LR: 0.000098 loss_total: 0.0974 loss_reconstruction: 0.0974 
2026-01-28 17:13:39,075 - training.train_lora - INFO - Epoch 17 [19/23] Step 410 LR: 0.000098 loss_total: 0.1239 loss_reconstruction: 0.1239 
2026-01-28 17:14:05,081 - training.train_lora - INFO - Epoch 17 completed in 66.82s - Loss: 0.1088
2026-01-28 17:14:20,307 - training.train_lora - INFO - Epoch 18 [6/23] Step 420 LR: 0.000098 loss_total: 0.1032 loss_reconstruction: 0.1032 
2026-01-28 17:14:39,402 - training.train_lora - INFO - Epoch 18 [16/23] Step 430 LR: 0.000098 loss_total: 0.0580 loss_reconstruction: 0.0580 
2026-01-28 17:15:10,212 - training.train_lora - INFO - Epoch 18 completed in 65.13s - Loss: 0.1170
2026-01-28 17:15:19,958 - training.train_lora - INFO - Epoch 19 [3/23] Step 440 LR: 0.000098 loss_total: 0.1050 loss_reconstruction: 0.1050 
2026-01-28 17:15:37,548 - training.train_lora - INFO - Epoch 19 [13/23] Step 450 LR: 0.000098 loss_total: 0.0481 loss_reconstruction: 0.0481 
2026-01-28 17:16:13,339 - training.train_lora - INFO - Epoch 19 completed in 63.13s - Loss: 0.1036
2026-01-28 17:16:13,356 - training.train_lora - INFO - Saved LoRA checkpoint: ./output/checkpoints/lora_epoch_19.pth
2026-01-28 17:16:13,364 - training.train_lora - INFO - Saved best model: ./output/checkpoints/lora_best.pth
2026-01-28 17:16:17,342 - training.train_lora - INFO - Epoch 20 [0/23] Step 460 LR: 0.000098 loss_total: 0.0340 loss_reconstruction: 0.0340 
2026-01-28 17:16:37,190 - training.train_lora - INFO - Epoch 20 [10/23] Step 470 LR: 0.000098 loss_total: 0.1359 loss_reconstruction: 0.1359 
2026-01-28 17:16:56,625 - training.train_lora - INFO - Epoch 20 [20/23] Step 480 LR: 0.000098 loss_total: 0.0599 loss_reconstruction: 0.0599 
2026-01-28 17:17:20,491 - training.train_lora - INFO - Epoch 20 completed in 67.12s - Loss: 0.1106
2026-01-28 17:17:37,934 - training.train_lora - INFO - Epoch 21 [7/23] Step 490 LR: 0.000098 loss_total: 0.1397 loss_reconstruction: 0.1397 
2026-01-28 17:17:57,630 - training.train_lora - INFO - Epoch 21 [17/23] Step 500 LR: 0.000098 loss_total: 0.1328 loss_reconstruction: 0.1328 
2026-01-28 17:18:27,310 - training.train_lora - INFO - Epoch 21 completed in 66.82s - Loss: 0.1106
2026-01-28 17:18:39,065 - training.train_lora - INFO - Epoch 22 [4/23] Step 510 LR: 0.000098 loss_total: 0.1092 loss_reconstruction: 0.1092 
2026-01-28 17:18:58,884 - training.train_lora - INFO - Epoch 22 [14/23] Step 520 LR: 0.000098 loss_total: 0.0857 loss_reconstruction: 0.0857 
2026-01-28 17:19:34,630 - training.train_lora - INFO - Epoch 22 completed in 67.32s - Loss: 0.1014
2026-01-28 17:19:40,473 - training.train_lora - INFO - Epoch 23 [1/23] Step 530 LR: 0.000098 loss_total: 0.1158 loss_reconstruction: 0.1158 
2026-01-28 17:20:00,529 - training.train_lora - INFO - Epoch 23 [11/23] Step 540 LR: 0.000098 loss_total: 0.0577 loss_reconstruction: 0.0577 
2026-01-28 17:20:20,423 - training.train_lora - INFO - Epoch 23 [21/23] Step 550 LR: 0.000098 loss_total: 0.0912 loss_reconstruction: 0.0912 
2026-01-28 17:20:42,430 - training.train_lora - INFO - Epoch 23 completed in 67.80s - Loss: 0.1125
2026-01-28 17:21:02,213 - training.train_lora - INFO - Epoch 24 [8/23] Step 560 LR: 0.000098 loss_total: 0.1044 loss_reconstruction: 0.1044 
2026-01-28 17:21:21,899 - training.train_lora - INFO - Epoch 24 [18/23] Step 570 LR: 0.000098 loss_total: 0.1465 loss_reconstruction: 0.1465 
2026-01-28 17:21:49,799 - training.train_lora - INFO - Epoch 24 completed in 67.37s - Loss: 0.1127
2026-01-28 17:22:03,646 - training.train_lora - INFO - Epoch 25 [5/23] Step 580 LR: 0.000098 loss_total: 0.2022 loss_reconstruction: 0.2022 
2026-01-28 17:22:23,609 - training.train_lora - INFO - Epoch 25 [15/23] Step 590 LR: 0.000098 loss_total: 0.1701 loss_reconstruction: 0.1701 
2026-01-28 17:22:57,527 - training.train_lora - INFO - Epoch 25 completed in 67.73s - Loss: 0.1150
2026-01-28 17:23:05,355 - training.train_lora - INFO - Epoch 26 [2/23] Step 600 LR: 0.000097 loss_total: 0.1192 loss_reconstruction: 0.1192 
2026-01-28 17:23:25,322 - training.train_lora - INFO - Epoch 26 [12/23] Step 610 LR: 0.000097 loss_total: 0.1641 loss_reconstruction: 0.1641 
2026-01-28 17:23:44,969 - training.train_lora - INFO - Epoch 26 [22/23] Step 620 LR: 0.000097 loss_total: 0.1452 loss_reconstruction: 0.1452 
2026-01-28 17:24:04,977 - training.train_lora - INFO - Epoch 26 completed in 67.45s - Loss: 0.1090
2026-01-28 17:24:26,733 - training.train_lora - INFO - Epoch 27 [9/23] Step 630 LR: 0.000097 loss_total: 0.1378 loss_reconstruction: 0.1378 
2026-01-28 17:24:46,588 - training.train_lora - INFO - Epoch 27 [19/23] Step 640 LR: 0.000097 loss_total: 0.0458 loss_reconstruction: 0.0458 
2026-01-28 17:25:12,563 - training.train_lora - INFO - Epoch 27 completed in 67.59s - Loss: 0.1108
2026-01-28 17:25:28,527 - training.train_lora - INFO - Epoch 28 [6/23] Step 650 LR: 0.000097 loss_total: 0.0598 loss_reconstruction: 0.0598 
2026-01-28 17:25:48,558 - training.train_lora - INFO - Epoch 28 [16/23] Step 660 LR: 0.000097 loss_total: 0.1310 loss_reconstruction: 0.1310 
2026-01-28 17:26:20,846 - training.train_lora - INFO - Epoch 28 completed in 68.28s - Loss: 0.1131
2026-01-28 17:26:30,622 - training.train_lora - INFO - Epoch 29 [3/23] Step 670 LR: 0.000097 loss_total: 0.1540 loss_reconstruction: 0.1540 
2026-01-28 17:26:50,591 - training.train_lora - INFO - Epoch 29 [13/23] Step 680 LR: 0.000097 loss_total: 0.0966 loss_reconstruction: 0.0966 
2026-01-28 17:27:28,565 - training.train_lora - INFO - Epoch 29 completed in 67.72s - Loss: 0.1149
2026-01-28 17:27:28,584 - training.train_lora - INFO - Saved LoRA checkpoint: ./output/checkpoints/lora_epoch_29.pth
2026-01-28 17:27:32,306 - training.train_lora - INFO - Epoch 30 [0/23] Step 690 LR: 0.000097 loss_total: 0.0691 loss_reconstruction: 0.0691 
2026-01-28 17:27:51,994 - training.train_lora - INFO - Epoch 30 [10/23] Step 700 LR: 0.000097 loss_total: 0.1011 loss_reconstruction: 0.1011 
2026-01-28 17:28:11,714 - training.train_lora - INFO - Epoch 30 [20/23] Step 710 LR: 0.000097 loss_total: 0.0305 loss_reconstruction: 0.0305 
2026-01-28 17:28:35,629 - training.train_lora - INFO - Epoch 30 completed in 67.04s - Loss: 0.1125
2026-01-28 17:28:53,405 - training.train_lora - INFO - Epoch 31 [7/23] Step 720 LR: 0.000097 loss_total: 0.1438 loss_reconstruction: 0.1438 
2026-01-28 17:29:13,183 - training.train_lora - INFO - Epoch 31 [17/23] Step 730 LR: 0.000097 loss_total: 0.1176 loss_reconstruction: 0.1176 
2026-01-28 17:29:43,041 - training.train_lora - INFO - Epoch 31 completed in 67.41s - Loss: 0.1032
2026-01-28 17:29:54,769 - training.train_lora - INFO - Epoch 32 [4/23] Step 740 LR: 0.000097 loss_total: 0.0606 loss_reconstruction: 0.0606 
2026-01-28 17:30:14,573 - training.train_lora - INFO - Epoch 32 [14/23] Step 750 LR: 0.000097 loss_total: 0.0712 loss_reconstruction: 0.0712 
2026-01-28 17:30:50,284 - training.train_lora - INFO - Epoch 32 completed in 67.24s - Loss: 0.1089
2026-01-28 17:30:56,160 - training.train_lora - INFO - Epoch 33 [1/23] Step 760 LR: 0.000097 loss_total: 0.1156 loss_reconstruction: 0.1156 
2026-01-28 17:31:16,078 - training.train_lora - INFO - Epoch 33 [11/23] Step 770 LR: 0.000097 loss_total: 0.0988 loss_reconstruction: 0.0988 
2026-01-28 17:31:35,819 - training.train_lora - INFO - Epoch 33 [21/23] Step 780 LR: 0.000097 loss_total: 0.2555 loss_reconstruction: 0.2555 
2026-01-28 17:31:57,811 - training.train_lora - INFO - Epoch 33 completed in 67.53s - Loss: 0.1168
2026-01-28 17:32:17,446 - training.train_lora - INFO - Epoch 34 [8/23] Step 790 LR: 0.000097 loss_total: 0.1633 loss_reconstruction: 0.1633 
2026-01-28 17:32:37,176 - training.train_lora - INFO - Epoch 34 [18/23] Step 800 LR: 0.000097 loss_total: 0.0999 loss_reconstruction: 0.0999 
2026-01-28 17:33:04,950 - training.train_lora - INFO - Epoch 34 completed in 67.14s - Loss: 0.1123
2026-01-28 17:33:18,926 - training.train_lora - INFO - Epoch 35 [5/23] Step 810 LR: 0.000097 loss_total: 0.0661 loss_reconstruction: 0.0661 
2026-01-28 17:33:38,891 - training.train_lora - INFO - Epoch 35 [15/23] Step 820 LR: 0.000097 loss_total: 0.0884 loss_reconstruction: 0.0884 
2026-01-28 17:34:12,833 - training.train_lora - INFO - Epoch 35 completed in 67.88s - Loss: 0.1095
2026-01-28 17:34:20,669 - training.train_lora - INFO - Epoch 36 [2/23] Step 830 LR: 0.000096 loss_total: 0.1126 loss_reconstruction: 0.1126 
2026-01-28 17:34:40,578 - training.train_lora - INFO - Epoch 36 [12/23] Step 840 LR: 0.000096 loss_total: 0.0740 loss_reconstruction: 0.0740 
2026-01-28 17:35:00,457 - training.train_lora - INFO - Epoch 36 [22/23] Step 850 LR: 0.000096 loss_total: 0.1312 loss_reconstruction: 0.1312 
2026-01-28 17:35:20,464 - training.train_lora - INFO - Epoch 36 completed in 67.63s - Loss: 0.1063
2026-01-28 17:35:42,376 - training.train_lora - INFO - Epoch 37 [9/23] Step 860 LR: 0.000096 loss_total: 0.0604 loss_reconstruction: 0.0604 
2026-01-28 17:36:02,200 - training.train_lora - INFO - Epoch 37 [19/23] Step 870 LR: 0.000096 loss_total: 0.1081 loss_reconstruction: 0.1081 
2026-01-28 17:36:28,181 - training.train_lora - INFO - Epoch 37 completed in 67.72s - Loss: 0.1045
2026-01-28 17:36:44,083 - training.train_lora - INFO - Epoch 38 [6/23] Step 880 LR: 0.000096 loss_total: 0.1114 loss_reconstruction: 0.1114 
2026-01-28 17:37:03,943 - training.train_lora - INFO - Epoch 38 [16/23] Step 890 LR: 0.000096 loss_total: 0.1687 loss_reconstruction: 0.1687 
2026-01-28 17:37:35,768 - training.train_lora - INFO - Epoch 38 completed in 67.59s - Loss: 0.1041
2026-01-28 17:37:45,698 - training.train_lora - INFO - Epoch 39 [3/23] Step 900 LR: 0.000096 loss_total: 0.2061 loss_reconstruction: 0.2061 
2026-01-28 17:38:05,787 - training.train_lora - INFO - Epoch 39 [13/23] Step 910 LR: 0.000096 loss_total: 0.1098 loss_reconstruction: 0.1098 
2026-01-28 17:38:43,924 - training.train_lora - INFO - Epoch 39 completed in 68.15s - Loss: 0.1188
2026-01-28 17:38:43,943 - training.train_lora - INFO - Saved LoRA checkpoint: ./output/checkpoints/lora_epoch_39.pth
2026-01-28 17:38:47,670 - training.train_lora - INFO - Epoch 40 [0/23] Step 920 LR: 0.000096 loss_total: 0.1176 loss_reconstruction: 0.1176 
2026-01-28 17:39:07,736 - training.train_lora - INFO - Epoch 40 [10/23] Step 930 LR: 0.000096 loss_total: 0.0763 loss_reconstruction: 0.0763 
2026-01-28 17:39:27,532 - training.train_lora - INFO - Epoch 40 [20/23] Step 940 LR: 0.000096 loss_total: 0.1363 loss_reconstruction: 0.1363 
2026-01-28 17:39:51,564 - training.train_lora - INFO - Epoch 40 completed in 67.61s - Loss: 0.1009
2026-01-28 17:40:09,300 - training.train_lora - INFO - Epoch 41 [7/23] Step 950 LR: 0.000096 loss_total: 0.1306 loss_reconstruction: 0.1306 
2026-01-28 17:40:29,329 - training.train_lora - INFO - Epoch 41 [17/23] Step 960 LR: 0.000096 loss_total: 0.0750 loss_reconstruction: 0.0750 
2026-01-28 17:40:59,171 - training.train_lora - INFO - Epoch 41 completed in 67.61s - Loss: 0.1075
2026-01-28 17:41:11,024 - training.train_lora - INFO - Epoch 42 [4/23] Step 970 LR: 0.000096 loss_total: 0.1038 loss_reconstruction: 0.1038 
2026-01-28 17:41:31,015 - training.train_lora - INFO - Epoch 42 [14/23] Step 980 LR: 0.000096 loss_total: 0.1534 loss_reconstruction: 0.1534 
2026-01-28 17:42:06,835 - training.train_lora - INFO - Epoch 42 completed in 67.66s - Loss: 0.1142
2026-01-28 17:42:12,749 - training.train_lora - INFO - Epoch 43 [1/23] Step 990 LR: 0.000096 loss_total: 0.0917 loss_reconstruction: 0.0917 
2026-01-28 17:42:32,624 - training.train_lora - INFO - Epoch 43 [11/23] Step 1000 LR: 0.000096 loss_total: 0.1123 loss_reconstruction: 0.1123 
2026-01-28 17:42:52,493 - training.train_lora - INFO - Epoch 43 [21/23] Step 1010 LR: 0.000096 loss_total: 0.0676 loss_reconstruction: 0.0676 
2026-01-28 17:43:14,539 - training.train_lora - INFO - Epoch 43 completed in 67.70s - Loss: 0.1058
2026-01-28 17:43:34,725 - training.train_lora - INFO - Epoch 44 [8/23] Step 1020 LR: 0.000096 loss_total: 0.1003 loss_reconstruction: 0.1003 
2026-01-28 17:43:54,646 - training.train_lora - INFO - Epoch 44 [18/23] Step 1030 LR: 0.000096 loss_total: 0.2008 loss_reconstruction: 0.2008 
2026-01-28 17:44:22,471 - training.train_lora - INFO - Epoch 44 completed in 67.93s - Loss: 0.1068
2026-01-28 17:44:36,343 - training.train_lora - INFO - Epoch 45 [5/23] Step 1040 LR: 0.000096 loss_total: 0.1720 loss_reconstruction: 0.1720 
2026-01-28 17:44:56,886 - training.train_lora - INFO - Epoch 45 [15/23] Step 1050 LR: 0.000096 loss_total: 0.0874 loss_reconstruction: 0.0874 
2026-01-28 17:45:31,132 - training.train_lora - INFO - Epoch 45 completed in 68.66s - Loss: 0.1169
2026-01-28 17:45:38,958 - training.train_lora - INFO - Epoch 46 [2/23] Step 1060 LR: 0.000096 loss_total: 0.1640 loss_reconstruction: 0.1640 
2026-01-28 17:45:59,152 - training.train_lora - INFO - Epoch 46 [12/23] Step 1070 LR: 0.000096 loss_total: 0.1124 loss_reconstruction: 0.1124 
2026-01-28 17:46:19,466 - training.train_lora - INFO - Epoch 46 [22/23] Step 1080 LR: 0.000096 loss_total: 0.1342 loss_reconstruction: 0.1342 
2026-01-28 17:46:39,479 - training.train_lora - INFO - Epoch 46 completed in 68.35s - Loss: 0.1016
2026-01-28 17:47:02,318 - training.train_lora - INFO - Epoch 47 [9/23] Step 1090 LR: 0.000095 loss_total: 0.1187 loss_reconstruction: 0.1187 
2026-01-28 17:47:22,250 - training.train_lora - INFO - Epoch 47 [19/23] Step 1100 LR: 0.000095 loss_total: 0.0939 loss_reconstruction: 0.0939 
2026-01-28 17:47:48,269 - training.train_lora - INFO - Epoch 47 completed in 68.79s - Loss: 0.1083
2026-01-28 17:48:04,052 - training.train_lora - INFO - Epoch 48 [6/23] Step 1110 LR: 0.000095 loss_total: 0.1141 loss_reconstruction: 0.1141 
2026-01-28 17:48:24,029 - training.train_lora - INFO - Epoch 48 [16/23] Step 1120 LR: 0.000095 loss_total: 0.0718 loss_reconstruction: 0.0718 
2026-01-28 17:48:55,967 - training.train_lora - INFO - Epoch 48 completed in 67.70s - Loss: 0.1073
2026-01-28 17:49:05,709 - training.train_lora - INFO - Epoch 49 [3/23] Step 1130 LR: 0.000095 loss_total: 0.0660 loss_reconstruction: 0.0660 
2026-01-28 17:49:25,554 - training.train_lora - INFO - Epoch 49 [13/23] Step 1140 LR: 0.000095 loss_total: 0.0394 loss_reconstruction: 0.0394 
2026-01-28 17:50:03,326 - training.train_lora - INFO - Epoch 49 completed in 67.36s - Loss: 0.1202
2026-01-28 17:50:03,345 - training.train_lora - INFO - Saved LoRA checkpoint: ./output/checkpoints/lora_epoch_49.pth
2026-01-28 17:50:03,371 - training.train_lora - INFO - Saved LoRA checkpoint: ./output/checkpoints/lora_epoch_49.pth
2026-01-28 17:50:03,380 - training.train_lora - INFO - Training completed!
2026-01-28 17:50:03,382 - __main__ - INFO - 
============================================================
2026-01-28 17:50:03,382 - __main__ - INFO - Step 5: Saving final LoRA weights
2026-01-28 17:50:03,382 - __main__ - INFO - ============================================================
2026-01-28 17:50:03,382 - __main__ - ERROR - Training failed: save_lora_checkpoint() got an unexpected keyword argument 'metadata'
Traceback (most recent call last):
  File "/Users/mac-min1/Desktop/Venture_Project/Retrieval-based-Voice-Conversion-WebUI/LoraModel/scripts/train_lora_e2e.py", line 459, in main
    result = train_lora_e2e(
  File "/Users/mac-min1/Desktop/Venture_Project/Retrieval-based-Voice-Conversion-WebUI/LoraModel/scripts/train_lora_e2e.py", line 327, in train_lora_e2e
    save_lora_checkpoint(
TypeError: save_lora_checkpoint() got an unexpected keyword argument 'metadata'
