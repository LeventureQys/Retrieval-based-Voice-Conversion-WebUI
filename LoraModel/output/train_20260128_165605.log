2026-01-28 16:56:05,071 - __main__ - INFO - Log file: ./output/train_20260128_165605.log
2026-01-28 16:56:05,071 - __main__ - WARNING - CUDA not available, falling back to CPU
2026-01-28 16:56:05,071 - __main__ - INFO - ============================================================
2026-01-28 16:56:05,071 - __main__ - INFO - RVC-LoRA End-to-End Training
2026-01-28 16:56:05,071 - __main__ - INFO - ============================================================
2026-01-28 16:56:05,071 - __main__ - INFO - Input directory: ./download/base_voice
2026-01-28 16:56:05,071 - __main__ - INFO - Output directory: ./output
2026-01-28 16:56:05,071 - __main__ - INFO - Base model: ./download/pretrained_v2/f0G40k.pth
2026-01-28 16:56:05,071 - __main__ - INFO - Sample rate: 40000
2026-01-28 16:56:05,071 - __main__ - INFO - Version: v2
2026-01-28 16:56:05,071 - __main__ - INFO - Device: cpu
2026-01-28 16:56:05,071 - __main__ - INFO - LoRA rank: 8, alpha: 16
2026-01-28 16:56:05,071 - __main__ - INFO - ============================================================
2026-01-28 16:56:05,071 - __main__ - INFO - Skipping preprocessing, found 47 existing samples
2026-01-28 16:56:05,071 - __main__ - INFO - 
============================================================
2026-01-28 16:56:05,071 - __main__ - INFO - Step 2: Loading model and injecting LoRA
2026-01-28 16:56:05,071 - __main__ - INFO - ============================================================
2026-01-28 16:56:05,130 - models.synthesizer_lora - INFO - Using default config for pretrained model (sr=40000, version=v2)
2026-01-28 16:56:05,451 - models.synthesizer_lora - INFO - Injecting LoRA into Synthesizer...
2026-01-28 16:56:05,516 - models.synthesizer_lora - INFO - LoRA injected into dec (Generator)
2026-01-28 16:56:05,517 - models.synthesizer_lora - INFO - Frozen 36,417,922 parameters, 384,768 LoRA parameters trainable
2026-01-28 16:56:05,524 - __main__ - INFO - Total parameters: 36,802,690
2026-01-28 16:56:05,524 - __main__ - INFO - LoRA parameters: 384,768 (1.05%)
2026-01-28 16:56:05,524 - __main__ - INFO - 
============================================================
2026-01-28 16:56:05,524 - __main__ - INFO - Step 3: Creating data loader
2026-01-28 16:56:05,524 - __main__ - INFO - ============================================================
2026-01-28 16:56:05,524 - training.data_loader - INFO - Auto-detected preprocessed dataset in ./output/preprocessed/training_data
2026-01-28 16:56:05,524 - training.data_loader - INFO - PreprocessedDataset: Found 47 samples
2026-01-28 16:56:05,524 - __main__ - INFO - Data loader created: 23 batches
2026-01-28 16:56:05,524 - __main__ - INFO - 
============================================================
2026-01-28 16:56:05,524 - __main__ - INFO - Step 4: Training LoRA
2026-01-28 16:56:05,524 - __main__ - INFO - ============================================================
2026-01-28 16:56:05,941 - training.train_lora - INFO - Optimizer setup with 152 parameter groups
2026-01-28 16:56:05,942 - training.train_lora - INFO - Starting training from epoch 0
2026-01-28 16:56:05,942 - training.train_lora - INFO - Total epochs: 50
2026-01-28 16:56:05,942 - training.train_lora - INFO - Batch size: 2
2026-01-28 16:56:05,942 - training.train_lora - INFO - Learning rate: 0.0001
2026-01-28 16:56:08,882 - training.train_lora - INFO - Epoch 0 [0/23] Step 0 LR: 0.000100 loss_total: 0.1420 loss_reconstruction: 0.1420 
2026-01-28 16:56:25,767 - training.train_lora - INFO - Epoch 0 [10/23] Step 10 LR: 0.000100 loss_total: 0.0740 loss_reconstruction: 0.0740 
2026-01-28 16:56:40,401 - training.train_lora - INFO - Epoch 0 [20/23] Step 20 LR: 0.000100 loss_total: 0.1276 loss_reconstruction: 0.1276 
2026-01-28 16:57:02,937 - training.train_lora - INFO - Epoch 0 completed in 56.99s - Loss: 0.1268
2026-01-28 16:57:17,529 - training.train_lora - INFO - Epoch 1 [7/23] Step 30 LR: 0.000100 loss_total: 0.0807 loss_reconstruction: 0.0807 
2026-01-28 16:57:33,471 - training.train_lora - INFO - Epoch 1 [17/23] Step 40 LR: 0.000100 loss_total: 0.1103 loss_reconstruction: 0.1103 
2026-01-28 16:57:59,764 - training.train_lora - INFO - Epoch 1 completed in 56.83s - Loss: 0.1096
2026-01-28 16:58:09,587 - training.train_lora - INFO - Epoch 2 [4/23] Step 50 LR: 0.000100 loss_total: 0.1064 loss_reconstruction: 0.1064 
2026-01-28 16:58:26,229 - training.train_lora - INFO - Epoch 2 [14/23] Step 60 LR: 0.000100 loss_total: 0.1312 loss_reconstruction: 0.1312 
2026-01-28 16:58:57,045 - training.train_lora - INFO - Epoch 2 completed in 57.28s - Loss: 0.1182
2026-01-28 16:59:01,885 - training.train_lora - INFO - Epoch 3 [1/23] Step 70 LR: 0.000100 loss_total: 0.1433 loss_reconstruction: 0.1433 
2026-01-28 16:59:18,631 - training.train_lora - INFO - Epoch 3 [11/23] Step 80 LR: 0.000100 loss_total: 0.0763 loss_reconstruction: 0.0763 
2026-01-28 16:59:33,134 - training.train_lora - INFO - Epoch 3 [21/23] Step 90 LR: 0.000100 loss_total: 0.1390 loss_reconstruction: 0.1390 
2026-01-28 16:59:54,415 - training.train_lora - INFO - Epoch 3 completed in 57.37s - Loss: 0.1160
2026-01-28 17:00:10,978 - training.train_lora - INFO - Epoch 4 [8/23] Step 100 LR: 0.000100 loss_total: 0.1159 loss_reconstruction: 0.1159 
2026-01-28 17:00:26,661 - training.train_lora - INFO - Epoch 4 [18/23] Step 110 LR: 0.000100 loss_total: 0.1613 loss_reconstruction: 0.1613 
2026-01-28 17:00:51,694 - training.train_lora - INFO - Epoch 4 completed in 57.28s - Loss: 0.1175
2026-01-28 17:01:03,275 - training.train_lora - INFO - Epoch 5 [5/23] Step 120 LR: 0.000100 loss_total: 0.1482 loss_reconstruction: 0.1482 
2026-01-28 17:01:19,963 - training.train_lora - INFO - Epoch 5 [15/23] Step 130 LR: 0.000100 loss_total: 0.0618 loss_reconstruction: 0.0618 
2026-01-28 17:01:49,192 - training.train_lora - INFO - Epoch 5 completed in 57.50s - Loss: 0.1143
2026-01-28 17:01:55,725 - training.train_lora - INFO - Epoch 6 [2/23] Step 140 LR: 0.000099 loss_total: 0.1522 loss_reconstruction: 0.1522 
2026-01-28 17:02:12,167 - training.train_lora - INFO - Epoch 6 [12/23] Step 150 LR: 0.000099 loss_total: 0.1166 loss_reconstruction: 0.1166 
2026-01-28 17:02:26,794 - training.train_lora - INFO - Epoch 6 [22/23] Step 160 LR: 0.000099 loss_total: 0.0682 loss_reconstruction: 0.0682 
2026-01-28 17:02:46,800 - training.train_lora - INFO - Epoch 6 completed in 57.61s - Loss: 0.1115
2026-01-28 17:03:05,539 - training.train_lora - INFO - Epoch 7 [9/23] Step 170 LR: 0.000099 loss_total: 0.1272 loss_reconstruction: 0.1272 
2026-01-28 17:03:21,602 - training.train_lora - INFO - Epoch 7 [19/23] Step 180 LR: 0.000099 loss_total: 0.0983 loss_reconstruction: 0.0983 
2026-01-28 17:03:45,520 - training.train_lora - INFO - Epoch 7 completed in 58.72s - Loss: 0.1131
2026-01-28 17:03:58,956 - training.train_lora - INFO - Epoch 8 [6/23] Step 190 LR: 0.000099 loss_total: 0.1013 loss_reconstruction: 0.1013 
2026-01-28 17:04:15,798 - training.train_lora - INFO - Epoch 8 [16/23] Step 200 LR: 0.000099 loss_total: 0.1229 loss_reconstruction: 0.1229 
2026-01-28 17:04:44,481 - training.train_lora - INFO - Epoch 8 completed in 58.96s - Loss: 0.1013
2026-01-28 17:04:52,941 - training.train_lora - INFO - Epoch 9 [3/23] Step 210 LR: 0.000099 loss_total: 0.0912 loss_reconstruction: 0.0912 
2026-01-28 17:05:10,440 - training.train_lora - INFO - Epoch 9 [13/23] Step 220 LR: 0.000099 loss_total: 0.0666 loss_reconstruction: 0.0666 
2026-01-28 17:05:44,248 - training.train_lora - INFO - Epoch 9 completed in 59.77s - Loss: 0.1017
2026-01-28 17:05:44,265 - training.train_lora - INFO - Saved LoRA checkpoint: ./output/checkpoints/lora_epoch_9.pth
2026-01-28 17:05:44,272 - training.train_lora - INFO - Saved best model: ./output/checkpoints/lora_best.pth
2026-01-28 17:05:47,389 - training.train_lora - INFO - Epoch 10 [0/23] Step 230 LR: 0.000099 loss_total: 0.0511 loss_reconstruction: 0.0511 
2026-01-28 17:06:04,504 - training.train_lora - INFO - Epoch 10 [10/23] Step 240 LR: 0.000099 loss_total: 0.0577 loss_reconstruction: 0.0577 
2026-01-28 17:06:21,010 - training.train_lora - INFO - Epoch 10 [20/23] Step 250 LR: 0.000099 loss_total: 0.1085 loss_reconstruction: 0.1085 
2026-01-28 17:06:43,622 - training.train_lora - INFO - Epoch 10 completed in 59.34s - Loss: 0.1053
2026-01-28 17:06:59,372 - training.train_lora - INFO - Epoch 11 [7/23] Step 260 LR: 0.000099 loss_total: 0.0827 loss_reconstruction: 0.0827 
2026-01-28 17:07:15,941 - training.train_lora - INFO - Epoch 11 [17/23] Step 270 LR: 0.000099 loss_total: 0.0845 loss_reconstruction: 0.0845 
2026-01-28 17:07:43,566 - training.train_lora - INFO - Epoch 11 completed in 59.94s - Loss: 0.1170
2026-01-28 17:07:54,160 - training.train_lora - INFO - Epoch 12 [4/23] Step 280 LR: 0.000099 loss_total: 0.1229 loss_reconstruction: 0.1229 
2026-01-28 17:08:11,567 - training.train_lora - INFO - Epoch 12 [14/23] Step 290 LR: 0.000099 loss_total: 0.1082 loss_reconstruction: 0.1082 
2026-01-28 17:08:45,292 - training.train_lora - INFO - Epoch 12 completed in 61.73s - Loss: 0.1102
2026-01-28 17:08:50,353 - training.train_lora - INFO - Epoch 13 [1/23] Step 300 LR: 0.000099 loss_total: 0.0933 loss_reconstruction: 0.0933 
2026-01-28 17:09:09,220 - training.train_lora - INFO - Epoch 13 [11/23] Step 310 LR: 0.000099 loss_total: 0.1081 loss_reconstruction: 0.1081 
2026-01-28 17:09:27,657 - training.train_lora - INFO - Epoch 13 [21/23] Step 320 LR: 0.000099 loss_total: 0.1097 loss_reconstruction: 0.1097 
2026-01-28 17:09:48,924 - training.train_lora - INFO - Epoch 13 completed in 63.63s - Loss: 0.1126
2026-01-28 17:10:06,441 - training.train_lora - INFO - Epoch 14 [8/23] Step 330 LR: 0.000099 loss_total: 0.1408 loss_reconstruction: 0.1408 
2026-01-28 17:10:24,028 - training.train_lora - INFO - Epoch 14 [18/23] Step 340 LR: 0.000099 loss_total: 0.0506 loss_reconstruction: 0.0506 
2026-01-28 17:10:50,619 - training.train_lora - INFO - Epoch 14 completed in 61.69s - Loss: 0.1107
2026-01-28 17:11:03,797 - training.train_lora - INFO - Epoch 15 [5/23] Step 350 LR: 0.000099 loss_total: 0.1309 loss_reconstruction: 0.1309 
2026-01-28 17:11:23,145 - training.train_lora - INFO - Epoch 15 [15/23] Step 360 LR: 0.000099 loss_total: 0.1243 loss_reconstruction: 0.1243 
2026-01-28 17:11:56,144 - training.train_lora - INFO - Epoch 15 completed in 65.53s - Loss: 0.1130
2026-01-28 17:12:03,345 - training.train_lora - INFO - Epoch 16 [2/23] Step 370 LR: 0.000098 loss_total: 0.1908 loss_reconstruction: 0.1908 
2026-01-28 17:12:21,169 - training.train_lora - INFO - Epoch 16 [12/23] Step 380 LR: 0.000098 loss_total: 0.0997 loss_reconstruction: 0.0997 
2026-01-28 17:12:39,233 - training.train_lora - INFO - Epoch 16 [22/23] Step 390 LR: 0.000098 loss_total: 0.1460 loss_reconstruction: 0.1460 
2026-01-28 17:12:59,241 - training.train_lora - INFO - Epoch 16 completed in 63.10s - Loss: 0.1161
2026-01-28 17:13:19,836 - training.train_lora - INFO - Epoch 17 [9/23] Step 400 LR: 0.000098 loss_total: 0.1595 loss_reconstruction: 0.1595 
2026-01-28 17:13:39,122 - training.train_lora - INFO - Epoch 17 [19/23] Step 410 LR: 0.000098 loss_total: 0.1645 loss_reconstruction: 0.1645 
2026-01-28 17:14:04,940 - training.train_lora - INFO - Epoch 17 completed in 65.70s - Loss: 0.1102
2026-01-28 17:14:20,396 - training.train_lora - INFO - Epoch 18 [6/23] Step 420 LR: 0.000098 loss_total: 0.0471 loss_reconstruction: 0.0471 
2026-01-28 17:14:39,520 - training.train_lora - INFO - Epoch 18 [16/23] Step 430 LR: 0.000098 loss_total: 0.1425 loss_reconstruction: 0.1425 
2026-01-28 17:15:10,907 - training.train_lora - INFO - Epoch 18 completed in 65.97s - Loss: 0.1136
2026-01-28 17:15:20,212 - training.train_lora - INFO - Epoch 19 [3/23] Step 440 LR: 0.000098 loss_total: 0.0821 loss_reconstruction: 0.0821 
2026-01-28 17:15:38,469 - training.train_lora - INFO - Epoch 19 [13/23] Step 450 LR: 0.000098 loss_total: 0.0624 loss_reconstruction: 0.0624 
2026-01-28 17:16:14,288 - training.train_lora - INFO - Epoch 19 completed in 63.38s - Loss: 0.1107
2026-01-28 17:16:14,307 - training.train_lora - INFO - Saved LoRA checkpoint: ./output/checkpoints/lora_epoch_19.pth
2026-01-28 17:16:17,685 - training.train_lora - INFO - Epoch 20 [0/23] Step 460 LR: 0.000098 loss_total: 0.2629 loss_reconstruction: 0.2629 
2026-01-28 17:16:37,178 - training.train_lora - INFO - Epoch 20 [10/23] Step 470 LR: 0.000098 loss_total: 0.0272 loss_reconstruction: 0.0272 
2026-01-28 17:16:56,776 - training.train_lora - INFO - Epoch 20 [20/23] Step 480 LR: 0.000098 loss_total: 0.0884 loss_reconstruction: 0.0884 
2026-01-28 17:17:20,840 - training.train_lora - INFO - Epoch 20 completed in 66.52s - Loss: 0.1074
2026-01-28 17:17:38,021 - training.train_lora - INFO - Epoch 21 [7/23] Step 490 LR: 0.000098 loss_total: 0.0489 loss_reconstruction: 0.0489 
2026-01-28 17:17:57,619 - training.train_lora - INFO - Epoch 21 [17/23] Step 500 LR: 0.000098 loss_total: 0.1095 loss_reconstruction: 0.1095 
2026-01-28 17:18:27,638 - training.train_lora - INFO - Epoch 21 completed in 66.80s - Loss: 0.1216
2026-01-28 17:18:39,149 - training.train_lora - INFO - Epoch 22 [4/23] Step 510 LR: 0.000098 loss_total: 0.1929 loss_reconstruction: 0.1929 
2026-01-28 17:18:58,941 - training.train_lora - INFO - Epoch 22 [14/23] Step 520 LR: 0.000098 loss_total: 0.0972 loss_reconstruction: 0.0972 
2026-01-28 17:19:34,869 - training.train_lora - INFO - Epoch 22 completed in 67.23s - Loss: 0.1087
2026-01-28 17:19:40,511 - training.train_lora - INFO - Epoch 23 [1/23] Step 530 LR: 0.000098 loss_total: 0.1097 loss_reconstruction: 0.1097 
2026-01-28 17:20:00,674 - training.train_lora - INFO - Epoch 23 [11/23] Step 540 LR: 0.000098 loss_total: 0.1221 loss_reconstruction: 0.1221 
2026-01-28 17:20:20,540 - training.train_lora - INFO - Epoch 23 [21/23] Step 550 LR: 0.000098 loss_total: 0.1308 loss_reconstruction: 0.1308 
2026-01-28 17:20:42,622 - training.train_lora - INFO - Epoch 23 completed in 67.75s - Loss: 0.1148
2026-01-28 17:21:02,247 - training.train_lora - INFO - Epoch 24 [8/23] Step 560 LR: 0.000098 loss_total: 0.1392 loss_reconstruction: 0.1392 
2026-01-28 17:21:22,013 - training.train_lora - INFO - Epoch 24 [18/23] Step 570 LR: 0.000098 loss_total: 0.1238 loss_reconstruction: 0.1238 
2026-01-28 17:21:50,099 - training.train_lora - INFO - Epoch 24 completed in 67.48s - Loss: 0.1046
2026-01-28 17:22:03,783 - training.train_lora - INFO - Epoch 25 [5/23] Step 580 LR: 0.000098 loss_total: 0.1870 loss_reconstruction: 0.1870 
2026-01-28 17:22:23,654 - training.train_lora - INFO - Epoch 25 [15/23] Step 590 LR: 0.000098 loss_total: 0.1092 loss_reconstruction: 0.1092 
2026-01-28 17:22:57,834 - training.train_lora - INFO - Epoch 25 completed in 67.73s - Loss: 0.1161
2026-01-28 17:23:05,401 - training.train_lora - INFO - Epoch 26 [2/23] Step 600 LR: 0.000097 loss_total: 0.1026 loss_reconstruction: 0.1026 
2026-01-28 17:23:25,382 - training.train_lora - INFO - Epoch 26 [12/23] Step 610 LR: 0.000097 loss_total: 0.1134 loss_reconstruction: 0.1134 
2026-01-28 17:23:45,308 - training.train_lora - INFO - Epoch 26 [22/23] Step 620 LR: 0.000097 loss_total: 0.1579 loss_reconstruction: 0.1579 
2026-01-28 17:24:05,315 - training.train_lora - INFO - Epoch 26 completed in 67.48s - Loss: 0.1005
2026-01-28 17:24:26,827 - training.train_lora - INFO - Epoch 27 [9/23] Step 630 LR: 0.000097 loss_total: 0.1748 loss_reconstruction: 0.1748 
2026-01-28 17:24:46,723 - training.train_lora - INFO - Epoch 27 [19/23] Step 640 LR: 0.000097 loss_total: 0.1123 loss_reconstruction: 0.1123 
2026-01-28 17:25:12,896 - training.train_lora - INFO - Epoch 27 completed in 67.58s - Loss: 0.1116
2026-01-28 17:25:28,510 - training.train_lora - INFO - Epoch 28 [6/23] Step 650 LR: 0.000097 loss_total: 0.0591 loss_reconstruction: 0.0591 
2026-01-28 17:25:48,641 - training.train_lora - INFO - Epoch 28 [16/23] Step 660 LR: 0.000097 loss_total: 0.0869 loss_reconstruction: 0.0869 
2026-01-28 17:26:21,121 - training.train_lora - INFO - Epoch 28 completed in 68.22s - Loss: 0.1055
2026-01-28 17:26:30,694 - training.train_lora - INFO - Epoch 29 [3/23] Step 670 LR: 0.000097 loss_total: 0.0919 loss_reconstruction: 0.0919 
2026-01-28 17:26:50,662 - training.train_lora - INFO - Epoch 29 [13/23] Step 680 LR: 0.000097 loss_total: 0.1299 loss_reconstruction: 0.1299 
2026-01-28 17:27:28,781 - training.train_lora - INFO - Epoch 29 completed in 67.66s - Loss: 0.1112
2026-01-28 17:27:28,806 - training.train_lora - INFO - Saved LoRA checkpoint: ./output/checkpoints/lora_epoch_29.pth
2026-01-28 17:27:32,322 - training.train_lora - INFO - Epoch 30 [0/23] Step 690 LR: 0.000097 loss_total: 0.1314 loss_reconstruction: 0.1314 
2026-01-28 17:27:52,079 - training.train_lora - INFO - Epoch 30 [10/23] Step 700 LR: 0.000097 loss_total: 0.0851 loss_reconstruction: 0.0851 
2026-01-28 17:28:11,772 - training.train_lora - INFO - Epoch 30 [20/23] Step 710 LR: 0.000097 loss_total: 0.1223 loss_reconstruction: 0.1223 
2026-01-28 17:28:35,973 - training.train_lora - INFO - Epoch 30 completed in 67.16s - Loss: 0.1108
2026-01-28 17:28:53,540 - training.train_lora - INFO - Epoch 31 [7/23] Step 720 LR: 0.000097 loss_total: 0.0975 loss_reconstruction: 0.0975 
2026-01-28 17:29:13,219 - training.train_lora - INFO - Epoch 31 [17/23] Step 730 LR: 0.000097 loss_total: 0.1067 loss_reconstruction: 0.1067 
2026-01-28 17:29:43,105 - training.train_lora - INFO - Epoch 31 completed in 67.13s - Loss: 0.1020
2026-01-28 17:29:54,850 - training.train_lora - INFO - Epoch 32 [4/23] Step 740 LR: 0.000097 loss_total: 0.0795 loss_reconstruction: 0.0795 
2026-01-28 17:30:14,643 - training.train_lora - INFO - Epoch 32 [14/23] Step 750 LR: 0.000097 loss_total: 0.0752 loss_reconstruction: 0.0752 
2026-01-28 17:30:50,540 - training.train_lora - INFO - Epoch 32 completed in 67.43s - Loss: 0.1075
2026-01-28 17:30:56,211 - training.train_lora - INFO - Epoch 33 [1/23] Step 760 LR: 0.000097 loss_total: 0.1107 loss_reconstruction: 0.1107 
2026-01-28 17:31:16,175 - training.train_lora - INFO - Epoch 33 [11/23] Step 770 LR: 0.000097 loss_total: 0.0693 loss_reconstruction: 0.0693 
2026-01-28 17:31:35,883 - training.train_lora - INFO - Epoch 33 [21/23] Step 780 LR: 0.000097 loss_total: 0.1077 loss_reconstruction: 0.1077 
2026-01-28 17:31:58,022 - training.train_lora - INFO - Epoch 33 completed in 67.48s - Loss: 0.1126
2026-01-28 17:32:17,514 - training.train_lora - INFO - Epoch 34 [8/23] Step 790 LR: 0.000097 loss_total: 0.0868 loss_reconstruction: 0.0868 
2026-01-28 17:32:37,249 - training.train_lora - INFO - Epoch 34 [18/23] Step 800 LR: 0.000097 loss_total: 0.0504 loss_reconstruction: 0.0504 
2026-01-28 17:33:05,291 - training.train_lora - INFO - Epoch 34 completed in 67.27s - Loss: 0.1123
2026-01-28 17:33:18,992 - training.train_lora - INFO - Epoch 35 [5/23] Step 810 LR: 0.000097 loss_total: 0.1885 loss_reconstruction: 0.1885 
2026-01-28 17:33:38,955 - training.train_lora - INFO - Epoch 35 [15/23] Step 820 LR: 0.000097 loss_total: 0.1480 loss_reconstruction: 0.1480 
2026-01-28 17:34:13,139 - training.train_lora - INFO - Epoch 35 completed in 67.85s - Loss: 0.1223
2026-01-28 17:34:20,695 - training.train_lora - INFO - Epoch 36 [2/23] Step 830 LR: 0.000096 loss_total: 0.0711 loss_reconstruction: 0.0711 
2026-01-28 17:34:40,651 - training.train_lora - INFO - Epoch 36 [12/23] Step 840 LR: 0.000096 loss_total: 0.0985 loss_reconstruction: 0.0985 
2026-01-28 17:35:00,480 - training.train_lora - INFO - Epoch 36 [22/23] Step 850 LR: 0.000096 loss_total: 0.0752 loss_reconstruction: 0.0752 
2026-01-28 17:35:20,487 - training.train_lora - INFO - Epoch 36 completed in 67.35s - Loss: 0.0982
2026-01-28 17:35:42,528 - training.train_lora - INFO - Epoch 37 [9/23] Step 860 LR: 0.000096 loss_total: 0.1172 loss_reconstruction: 0.1172 
2026-01-28 17:36:02,219 - training.train_lora - INFO - Epoch 37 [19/23] Step 870 LR: 0.000096 loss_total: 0.1013 loss_reconstruction: 0.1013 
2026-01-28 17:36:28,128 - training.train_lora - INFO - Epoch 37 completed in 67.64s - Loss: 0.1079
2026-01-28 17:36:44,148 - training.train_lora - INFO - Epoch 38 [6/23] Step 880 LR: 0.000096 loss_total: 0.0690 loss_reconstruction: 0.0690 
2026-01-28 17:37:04,053 - training.train_lora - INFO - Epoch 38 [16/23] Step 890 LR: 0.000096 loss_total: 0.1131 loss_reconstruction: 0.1131 
2026-01-28 17:37:35,937 - training.train_lora - INFO - Epoch 38 completed in 67.81s - Loss: 0.1098
2026-01-28 17:37:45,699 - training.train_lora - INFO - Epoch 39 [3/23] Step 900 LR: 0.000096 loss_total: 0.1711 loss_reconstruction: 0.1711 
2026-01-28 17:38:05,873 - training.train_lora - INFO - Epoch 39 [13/23] Step 910 LR: 0.000096 loss_total: 0.0690 loss_reconstruction: 0.0690 
2026-01-28 17:38:44,085 - training.train_lora - INFO - Epoch 39 completed in 68.15s - Loss: 0.1065
2026-01-28 17:38:44,111 - training.train_lora - INFO - Saved LoRA checkpoint: ./output/checkpoints/lora_epoch_39.pth
2026-01-28 17:38:47,765 - training.train_lora - INFO - Epoch 40 [0/23] Step 920 LR: 0.000096 loss_total: 0.1244 loss_reconstruction: 0.1244 
2026-01-28 17:39:07,854 - training.train_lora - INFO - Epoch 40 [10/23] Step 930 LR: 0.000096 loss_total: 0.1079 loss_reconstruction: 0.1079 
2026-01-28 17:39:27,720 - training.train_lora - INFO - Epoch 40 [20/23] Step 940 LR: 0.000096 loss_total: 0.1039 loss_reconstruction: 0.1039 
2026-01-28 17:39:51,885 - training.train_lora - INFO - Epoch 40 completed in 67.76s - Loss: 0.1006
2026-01-28 17:40:09,474 - training.train_lora - INFO - Epoch 41 [7/23] Step 950 LR: 0.000096 loss_total: 0.1264 loss_reconstruction: 0.1264 
2026-01-28 17:40:29,369 - training.train_lora - INFO - Epoch 41 [17/23] Step 960 LR: 0.000096 loss_total: 0.1067 loss_reconstruction: 0.1067 
2026-01-28 17:40:59,504 - training.train_lora - INFO - Epoch 41 completed in 67.62s - Loss: 0.1126
2026-01-28 17:41:11,072 - training.train_lora - INFO - Epoch 42 [4/23] Step 970 LR: 0.000096 loss_total: 0.0699 loss_reconstruction: 0.0699 
2026-01-28 17:41:31,006 - training.train_lora - INFO - Epoch 42 [14/23] Step 980 LR: 0.000096 loss_total: 0.0765 loss_reconstruction: 0.0765 
2026-01-28 17:42:06,837 - training.train_lora - INFO - Epoch 42 completed in 67.33s - Loss: 0.1065
2026-01-28 17:42:12,818 - training.train_lora - INFO - Epoch 43 [1/23] Step 990 LR: 0.000096 loss_total: 0.1235 loss_reconstruction: 0.1235 
2026-01-28 17:42:32,726 - training.train_lora - INFO - Epoch 43 [11/23] Step 1000 LR: 0.000096 loss_total: 0.0617 loss_reconstruction: 0.0617 
2026-01-28 17:42:52,543 - training.train_lora - INFO - Epoch 43 [21/23] Step 1010 LR: 0.000096 loss_total: 0.0671 loss_reconstruction: 0.0671 
2026-01-28 17:43:14,665 - training.train_lora - INFO - Epoch 43 completed in 67.83s - Loss: 0.1094
2026-01-28 17:43:34,740 - training.train_lora - INFO - Epoch 44 [8/23] Step 1020 LR: 0.000096 loss_total: 0.0992 loss_reconstruction: 0.0992 
2026-01-28 17:43:54,714 - training.train_lora - INFO - Epoch 44 [18/23] Step 1030 LR: 0.000096 loss_total: 0.1847 loss_reconstruction: 0.1847 
2026-01-28 17:44:22,798 - training.train_lora - INFO - Epoch 44 completed in 68.13s - Loss: 0.1084
2026-01-28 17:44:36,458 - training.train_lora - INFO - Epoch 45 [5/23] Step 1040 LR: 0.000096 loss_total: 0.0952 loss_reconstruction: 0.0952 
2026-01-28 17:44:56,807 - training.train_lora - INFO - Epoch 45 [15/23] Step 1050 LR: 0.000096 loss_total: 0.0998 loss_reconstruction: 0.0998 
2026-01-28 17:45:31,213 - training.train_lora - INFO - Epoch 45 completed in 68.41s - Loss: 0.1110
2026-01-28 17:45:39,044 - training.train_lora - INFO - Epoch 46 [2/23] Step 1060 LR: 0.000096 loss_total: 0.1461 loss_reconstruction: 0.1461 
2026-01-28 17:45:59,252 - training.train_lora - INFO - Epoch 46 [12/23] Step 1070 LR: 0.000096 loss_total: 0.2103 loss_reconstruction: 0.2103 
2026-01-28 17:46:19,789 - training.train_lora - INFO - Epoch 46 [22/23] Step 1080 LR: 0.000096 loss_total: 0.1115 loss_reconstruction: 0.1115 
2026-01-28 17:46:39,800 - training.train_lora - INFO - Epoch 46 completed in 68.59s - Loss: 0.1079
2026-01-28 17:47:02,325 - training.train_lora - INFO - Epoch 47 [9/23] Step 1090 LR: 0.000095 loss_total: 0.1314 loss_reconstruction: 0.1314 
2026-01-28 17:47:22,345 - training.train_lora - INFO - Epoch 47 [19/23] Step 1100 LR: 0.000095 loss_total: 0.0818 loss_reconstruction: 0.0818 
2026-01-28 17:47:48,323 - training.train_lora - INFO - Epoch 47 completed in 68.52s - Loss: 0.1098
2026-01-28 17:48:04,131 - training.train_lora - INFO - Epoch 48 [6/23] Step 1110 LR: 0.000095 loss_total: 0.0799 loss_reconstruction: 0.0799 
2026-01-28 17:48:24,123 - training.train_lora - INFO - Epoch 48 [16/23] Step 1120 LR: 0.000095 loss_total: 0.1233 loss_reconstruction: 0.1233 
2026-01-28 17:48:56,268 - training.train_lora - INFO - Epoch 48 completed in 67.94s - Loss: 0.1053
2026-01-28 17:49:05,756 - training.train_lora - INFO - Epoch 49 [3/23] Step 1130 LR: 0.000095 loss_total: 0.1057 loss_reconstruction: 0.1057 
2026-01-28 17:49:25,590 - training.train_lora - INFO - Epoch 49 [13/23] Step 1140 LR: 0.000095 loss_total: 0.1452 loss_reconstruction: 0.1452 
2026-01-28 17:50:03,449 - training.train_lora - INFO - Epoch 49 completed in 67.18s - Loss: 0.1144
2026-01-28 17:50:03,465 - training.train_lora - INFO - Saved LoRA checkpoint: ./output/checkpoints/lora_epoch_49.pth
2026-01-28 17:50:03,489 - training.train_lora - INFO - Saved LoRA checkpoint: ./output/checkpoints/lora_epoch_49.pth
2026-01-28 17:50:03,497 - training.train_lora - INFO - Training completed!
2026-01-28 17:50:03,500 - __main__ - INFO - 
============================================================
2026-01-28 17:50:03,500 - __main__ - INFO - Step 5: Saving final LoRA weights
2026-01-28 17:50:03,500 - __main__ - INFO - ============================================================
2026-01-28 17:50:03,501 - __main__ - ERROR - Training failed: save_lora_checkpoint() got an unexpected keyword argument 'metadata'
Traceback (most recent call last):
  File "/Users/mac-min1/Desktop/Venture_Project/Retrieval-based-Voice-Conversion-WebUI/LoraModel/scripts/train_lora_e2e.py", line 459, in main
    result = train_lora_e2e(
  File "/Users/mac-min1/Desktop/Venture_Project/Retrieval-based-Voice-Conversion-WebUI/LoraModel/scripts/train_lora_e2e.py", line 327, in train_lora_e2e
    save_lora_checkpoint(
TypeError: save_lora_checkpoint() got an unexpected keyword argument 'metadata'
