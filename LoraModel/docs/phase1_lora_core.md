# é˜¶æ®µ 1: LoRA æ ¸å¿ƒå®ç°

## æ¦‚è¿°

**çŠ¶æ€**: âœ… å·²å®Œæˆ
**å¼€å§‹æ—¥æœŸ**: 2026-01-28
**å®Œæˆæ—¥æœŸ**: 2026-01-28

æœ¬é˜¶æ®µå®ç°äº† LoRA (Low-Rank Adaptation) çš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ LoRA å±‚ã€é…ç½®å’Œå·¥å…·å‡½æ•°ã€‚

---

## å®ç°çš„åŠŸèƒ½

### 1. LoRA é…ç½® (`lora_config.py`)

#### LoRAConfig ç±»
é…ç½® LoRA çš„æ‰€æœ‰è¶…å‚æ•°ï¼š

```python
@dataclass
class LoRAConfig:
    r: int = 8                    # LoRA rank
    lora_alpha: int = 16          # ç¼©æ”¾å› å­
    lora_dropout: float = 0.0     # Dropout æ¦‚ç‡
    target_modules: List[str]     # ç›®æ ‡æ¨¡å—
    merge_weights: bool = False   # æ˜¯å¦åˆå¹¶æƒé‡
    bias: str = "none"            # åç½®è®­ç»ƒæ¨¡å¼
```

#### é¢„å®šä¹‰é…ç½®
- `DEFAULT_CONFIG`: é»˜è®¤é…ç½® (r=8)
- `HIGH_QUALITY_CONFIG`: é«˜è´¨é‡é…ç½® (r=16)
- `FAST_CONFIG`: å¿«é€Ÿé…ç½® (r=4)
- `BALANCED_CONFIG`: å¹³è¡¡é…ç½® (r=8, dropout=0.05)

### 2. LoRA å±‚ (`lora_layer.py`)

#### LoRALayer (åŸºç±»)
æ‰€æœ‰ LoRA å±‚çš„åŸºç±»ï¼Œå®ç°ï¼š
- ä½ç§©åˆ†è§£: W = W0 + BA
- ç¼©æ”¾å› å­: scaling = lora_alpha / r
- æƒé‡åˆå¹¶/åˆ†ç¦»

#### LoRALinear
LoRA å¢å¼ºçš„çº¿æ€§å±‚ï¼š
```python
class LoRALinear(nn.Linear, LoRALayer):
    # æ·»åŠ  lora_A (r Ã— in_features) å’Œ lora_B (out_features Ã— r)
    # å‰å‘ä¼ æ’­: output = Linear(x) + (x @ A.T @ B.T) * scaling
```

#### LoRAConv1d
LoRA å¢å¼ºçš„ 1D å·ç§¯å±‚ï¼š
```python
class LoRAConv1d(nn.Conv1d, LoRALayer):
    # å°†å·ç§¯æ ¸å±•å¹³ååº”ç”¨ä½ç§©åˆ†è§£
    # é€‚ç”¨äº ResBlock ä¸­çš„å·ç§¯å±‚
```

#### LoRAConvTranspose1d
LoRA å¢å¼ºçš„ 1D è½¬ç½®å·ç§¯å±‚ï¼š
```python
class LoRAConvTranspose1d(nn.ConvTranspose1d, LoRALayer):
    # ä¸“é—¨ç”¨äº RVC çš„ä¸Šé‡‡æ ·å±‚
    # è¿™æ˜¯æœ€é‡è¦çš„å±‚ï¼Œç›´æ¥å½±å“éŸ³è´¨
```

### 3. LoRA å·¥å…·å‡½æ•° (`lora_utils.py`)

#### å‚æ•°ç®¡ç†
- `mark_only_lora_as_trainable()`: å†»ç»“åŸºç¡€æƒé‡ï¼Œåªè®­ç»ƒ LoRA
- `get_lora_parameters()`: è·å–æ‰€æœ‰ LoRA å‚æ•°
- `count_lora_parameters()`: ç»Ÿè®¡å‚æ•°æ•°é‡

#### æ³¨å…¥å’Œæå–
- `inject_lora()`: å°† LoRA æ³¨å…¥åˆ°æ¨¡å‹ä¸­
- `extract_lora_weights()`: æå– LoRA æƒé‡
- `load_lora_weights()`: åŠ è½½ LoRA æƒé‡

#### æƒé‡åˆå¹¶
- `merge_lora_weights()`: åˆå¹¶ LoRA åˆ°åŸºç¡€æƒé‡
- `unmerge_lora_weights()`: åˆ†ç¦» LoRA æƒé‡

#### æ£€æŸ¥ç‚¹ç®¡ç†
- `save_lora_checkpoint()`: ä¿å­˜ LoRA æ£€æŸ¥ç‚¹
- `load_lora_checkpoint()`: åŠ è½½ LoRA æ£€æŸ¥ç‚¹

#### è°ƒè¯•å·¥å…·
- `print_lora_info()`: æ‰“å° LoRA å±‚ä¿¡æ¯

---

## æŠ€æœ¯ç»†èŠ‚

### LoRA åŸç†

LoRA é€šè¿‡ä½ç§©åˆ†è§£æ¥å‡å°‘å¯è®­ç»ƒå‚æ•°ï¼š

```
åŸå§‹æƒé‡æ›´æ–°: Î”W âˆˆ R^(dÃ—k)
LoRA åˆ†è§£: Î”W = BÂ·A
  å…¶ä¸­ B âˆˆ R^(dÃ—r), A âˆˆ R^(rÃ—k), r << min(d,k)

å‚æ•°é‡å¯¹æ¯”:
  å®Œæ•´: d Ã— k
  LoRA: d Ã— r + r Ã— k = r(d + k)

å½“ r << min(d,k) æ—¶ï¼Œå‚æ•°é‡å¤§å¹…å‡å°‘
```

### åˆå§‹åŒ–ç­–ç•¥

```python
# A çŸ©é˜µ: Kaiming uniform åˆå§‹åŒ–
nn.init.kaiming_uniform_(lora_A, a=math.sqrt(5))

# B çŸ©é˜µ: é›¶åˆå§‹åŒ–
nn.init.zeros_(lora_B)

# è¿™ç¡®ä¿åˆå§‹æ—¶ LoRA è´¡çŒ®ä¸ºé›¶: B @ A = 0
```

### ç¼©æ”¾å› å­

```python
scaling = lora_alpha / r

# ä½œç”¨: æ§åˆ¶ LoRA çš„å½±å“å¼ºåº¦
# é€šå¸¸è®¾ç½® lora_alpha = 2 * r
# è¿™æ · scaling = 2ï¼Œç»™ LoRA è¶³å¤Ÿçš„å­¦ä¹ èƒ½åŠ›
```

### æƒé‡åˆå¹¶

è®­ç»ƒæ—¶:
```python
output = base_forward(x) + lora_forward(x) * scaling
```

æ¨ç†æ—¶ï¼ˆåˆå¹¶åï¼‰:
```python
W_merged = W_base + (B @ A) * scaling
output = merged_forward(x)  # æ— é¢å¤–å¼€é”€
```

---

## ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºæœ¬ä½¿ç”¨

```python
from lora import LoRAConfig, inject_lora

# åˆ›å»ºé…ç½®
config = LoRAConfig(
    r=8,
    lora_alpha=16,
    target_modules=["ups", "resblocks"]
)

# æ³¨å…¥ LoRA
model = inject_lora(model, config)

# åªè®­ç»ƒ LoRA å‚æ•°
optimizer = torch.optim.AdamW(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=1e-4
)
```

### 2. ä¿å­˜å’ŒåŠ è½½

```python
from lora import save_lora_checkpoint, load_lora_checkpoint

# ä¿å­˜
save_lora_checkpoint(
    model=model,
    path="lora_checkpoint.pth",
    config=config,
    optimizer_state=optimizer.state_dict(),
    epoch=100
)

# åŠ è½½
model, opt_state, epoch, config = load_lora_checkpoint(
    model=model,
    path="lora_checkpoint.pth",
    load_optimizer=True
)
```

### 3. æƒé‡åˆå¹¶

```python
from lora import merge_lora_weights

# åˆå¹¶ç”¨äºæ¨ç†
model = merge_lora_weights(model)
model.eval()

# æ¨ç†æ—¶æ— é¢å¤–å¼€é”€
with torch.no_grad():
    output = model(input)
```

---

## æµ‹è¯•

### å•å…ƒæµ‹è¯•

åˆ›å»ºäº†ä»¥ä¸‹æµ‹è¯•æ–‡ä»¶ï¼ˆå¾…å®ç°ï¼‰:
- `tests/test_lora_layer.py`: æµ‹è¯• LoRA å±‚
- `tests/test_lora_config.py`: æµ‹è¯•é…ç½®
- `tests/test_lora_utils.py`: æµ‹è¯•å·¥å…·å‡½æ•°

### æµ‹è¯•ç”¨ä¾‹

```python
# æµ‹è¯• LoRA å±‚åˆå§‹åŒ–
def test_lora_linear_init():
    layer = LoRALinear(128, 256, r=8)
    assert layer.lora_A.shape == (8, 128)
    assert layer.lora_B.shape == (256, 8)
    assert torch.allclose(layer.lora_B, torch.zeros_like(layer.lora_B))

# æµ‹è¯•å‰å‘ä¼ æ’­
def test_lora_forward():
    layer = LoRALinear(128, 256, r=8)
    x = torch.randn(4, 128)
    output = layer(x)
    assert output.shape == (4, 256)

# æµ‹è¯•æƒé‡åˆå¹¶
def test_merge_weights():
    layer = LoRALinear(128, 256, r=8, merge_weights=True)
    layer.train()  # æœªåˆå¹¶
    assert not layer.merged
    layer.eval()   # åˆå¹¶
    assert layer.merged
```

---

## æ€§èƒ½åˆ†æ

### å‚æ•°é‡å¯¹æ¯”

ä»¥ RVC Generator ä¸ºä¾‹ (æ€»å‚æ•° 7.6M):

| å±‚ç±»å‹ | åŸå§‹å‚æ•° | LoRA (r=8) | å‡å°‘æ¯”ä¾‹ |
|--------|---------|-----------|---------|
| ä¸Šé‡‡æ ·å±‚ (5å±‚) | 2.5M | 80K | 96.8% |
| ResBlock (15å±‚) | 3.8M | 240K | 93.7% |
| å…¶ä»– | 1.3M | 0 | 100% |
| **æ€»è®¡** | **7.6M** | **320K** | **95.8%** |

### å†…å­˜å ç”¨

```
å®Œæ•´æ¨¡å‹: 7.6M Ã— 4 bytes = 30.4 MB
LoRA: 320K Ã— 4 bytes = 1.28 MB

èŠ‚çœ: 96% å†…å­˜
```

### è®¡ç®—å¼€é”€

è®­ç»ƒæ—¶:
```
å‰å‘ä¼ æ’­: base_forward + lora_forward
é¢å¤–å¼€é”€: ~5-10%
```

æ¨ç†æ—¶ï¼ˆåˆå¹¶åï¼‰:
```
å‰å‘ä¼ æ’­: merged_forward
é¢å¤–å¼€é”€: 0%
```

---

## å·²çŸ¥é—®é¢˜å’Œé™åˆ¶

### 1. ConvTranspose1d çš„ LoRA å®ç°

å½“å‰å®ç°ä½¿ç”¨äº†ç®€åŒ–çš„æ–¹æ³•ï¼š
```python
# åœ¨è¾“å…¥ç©ºé—´åº”ç”¨ LoRAï¼Œç„¶åæ’å€¼åˆ°è¾“å‡ºå¤§å°
# è¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼ï¼Œå¯èƒ½ä¸æ˜¯æœ€ä¼˜çš„
```

**æ”¹è¿›æ–¹å‘**: ç ”ç©¶æ›´ç²¾ç¡®çš„è½¬ç½®å·ç§¯ LoRA å®ç°

### 2. Weight Norm å…¼å®¹æ€§

RVC ä½¿ç”¨ `weight_norm` åŒ…è£…å·ç§¯å±‚ï¼Œå½“å‰å®ç°ï¼š
- å‡è®¾åœ¨æ³¨å…¥ LoRA å‰å·²ç§»é™¤ weight_norm
- æˆ–è€… LoRA åœ¨ weight_norm ä¹‹ååº”ç”¨

**æ”¹è¿›æ–¹å‘**: æ·»åŠ è‡ªåŠ¨å¤„ç† weight_norm çš„é€»è¾‘

### 3. åˆ†å¸ƒå¼è®­ç»ƒ

å½“å‰æœªæµ‹è¯•åˆ†å¸ƒå¼è®­ç»ƒå…¼å®¹æ€§ã€‚

**æ”¹è¿›æ–¹å‘**: æµ‹è¯• DDP å’Œ LoRA çš„å…¼å®¹æ€§

---

## ä¸‹ä¸€æ­¥

### é˜¶æ®µ 2: æ¨¡å‹é›†æˆ

1. å¤åˆ¶ RVC Generator ä»£ç 
2. åˆ›å»º GeneratorLoRA ç±»
3. å¤„ç† weight_norm å…¼å®¹æ€§
4. æµ‹è¯•å‰å‘ä¼ æ’­

### å¾…åŠäº‹é¡¹

- [ ] å®ç°å•å…ƒæµ‹è¯•
- [ ] ä¼˜åŒ– ConvTranspose1d LoRA
- [ ] æ·»åŠ  weight_norm è‡ªåŠ¨å¤„ç†
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] æ·»åŠ æ›´å¤šæ–‡æ¡£å’Œç¤ºä¾‹

---

## å‚è€ƒèµ„æ–™

### è®ºæ–‡
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)

### å®ç°å‚è€ƒ
- [Hugging Face PEFT](https://github.com/huggingface/peft)
- [Microsoft LoRA](https://github.com/microsoft/LoRA)

---

## æ›´æ–°æ—¥å¿—

### 2026-01-28
- âœ… åˆ›å»ºé¡¹ç›®ç»“æ„
- âœ… å®ç° LoRAConfig
- âœ… å®ç° LoRALayer åŸºç±»
- âœ… å®ç° LoRALinear
- âœ… å®ç° LoRAConv1d
- âœ… å®ç° LoRAConvTranspose1d
- âœ… å®ç°æ‰€æœ‰å·¥å…·å‡½æ•°
- âœ… ç¼–å†™é˜¶æ®µæ–‡æ¡£

**é˜¶æ®µ 1 å®Œæˆï¼** ğŸ‰

---

**ä¸‹ä¸€é˜¶æ®µ**: [é˜¶æ®µ 2 - æ¨¡å‹é›†æˆ](phase2_model_integration.md)
